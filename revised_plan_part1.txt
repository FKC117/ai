# REVISED AI-Powered Analytics System Implementation Guide - PART 1
# DataFlow Analytics - LangChain + Gemini 1.5 Flash Integration with Caching
# Version: 2.0

================================================================================
PHASE 1: ENVIRONMENT SETUP & DEPENDENCIES
================================================================================

STEP 1.1: Environment Configuration
Update .env file in ai/ directory:
```
SECRET_KEY=your_django_secret_key
GOOGLE_API_KEY=your_gemini_api_key_here
DEBUG=True
REDIS_URL=redis://localhost:6379/0
CACHE_TTL=3600
```

STEP 1.2: Additional Dependencies
Add to requirements.txt:
```
redis==5.0.1
django-redis==5.4.0
celery==5.3.4
scikit-learn==1.7.1
scipy==1.10.1
seaborn==0.12.2
plotly==5.18.0
statsmodels==0.14.1
```

================================================================================
PHASE 2: PROJECT STRUCTURE SETUP
================================================================================

STEP 2.1: Create Enhanced AI Module Structure
```
ai/analyticabd/
├── ai/
│   ├── __init__.py
│   ├── llm_client.py
│   ├── prompt_templates.py
│   ├── conversation_manager.py
│   ├── tool_executor.py
│   ├── cache_manager.py
│   └── async_processor.py
├── tools/
│   ├── __init__.py
│   ├── base_tool.py
│   ├── summary_statistics_tool.py
│   ├── correlation_tool.py
│   ├── regression_tool.py
│   ├── clustering_tool.py
│   ├── time_series_tool.py
│   ├── outlier_detection_tool.py
│   ├── hypothesis_testing_tool.py
│   ├── data_quality_tool.py
│   ├── visualization_tool.py
│   └── tool_registry.py
├── models/
│   ├── __init__.py
│   ├── chat_models.py
│   ├── cache_models.py
│   └── analysis_models.py
├── api/
│   ├── __init__.py
│   ├── serializers.py
│   ├── viewsets.py
│   └── routers.py
└── tasks/
    ├── __init__.py
    ├── celery.py
    └── analysis_tasks.py
```

================================================================================
PHASE 3: CACHING INFRASTRUCTURE
================================================================================

STEP 3.1: Cache Configuration (ai/ai/settings.py)
Add to settings.py:
```python
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': os.getenv('REDIS_URL', 'redis://localhost:6379/1'),
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
        },
        'KEY_PREFIX': 'dataflow_analytics',
        'TIMEOUT': int(os.getenv('CACHE_TTL', 3600)),
    }
}

LLM_CACHE_TIMEOUT = 1800
TOOL_RESULTS_CACHE_TIMEOUT = 7200
USER_SESSION_CACHE_TIMEOUT = 3600
```

STEP 3.2: Cache Manager (ai/analyticabd/ai/cache_manager.py)
```python
import hashlib
import json
from django.core.cache import cache
from django.conf import settings

class CacheManager:
    def __init__(self):
        self.default_timeout = settings.LLM_CACHE_TIMEOUT
    
    def generate_cache_key(self, prefix, data):
        data_str = json.dumps(data, sort_keys=True)
        return f"{prefix}:{hashlib.md5(data_str.encode()).hexdigest()}"
    
    def get_cached_response(self, cache_key):
        return cache.get(cache_key)
    
    def set_cached_response(self, cache_key, data, timeout=None):
        timeout = timeout or self.default_timeout
        cache.set(cache_key, data, timeout)
    
    def cache_tool_results(self, tool_name, dataset_id, parameters, results):
        cache_key = self.generate_cache_key(f"tool:{tool_name}", {
            'dataset_id': dataset_id,
            'parameters': parameters
        })
        cache.set(cache_key, results, settings.TOOL_RESULTS_CACHE_TIMEOUT)
        return cache_key
    
    def get_cached_tool_results(self, tool_name, dataset_id, parameters):
        cache_key = self.generate_cache_key(f"tool:{tool_name}", {
            'dataset_id': dataset_id,
            'parameters': parameters
        })
        return cache.get(cache_key)
```

================================================================================
PHASE 4: CORE TOOL FRAMEWORK
================================================================================

STEP 4.1: Base Tool Class (ai/analyticabd/tools/base_tool.py)
```python
from abc import ABC, abstractmethod
import pandas as pd
from typing import Dict, Any
from ..ai.cache_manager import CacheManager

class BaseTool(ABC):
    def __init__(self, dataset_id, user_id):
        self.dataset_id = dataset_id
        self.user_id = user_id
        self.dataset = None
        self.cache_manager = CacheManager()
        self.tool_name = self.__class__.__name__.lower()
    
    @abstractmethod
    def execute(self, parameters=None):
        pass
    
    @abstractmethod
    def get_description(self):
        pass
    
    @abstractmethod
    def get_parameters_schema(self):
        pass
    
    def load_dataset(self):
        from ..models import UserDataset
        cache_key = f"dataset:{self.dataset_id}"
        cached_dataset = self.cache_manager.get_cached_response(cache_key)
        
        if cached_dataset:
            self.dataset = pd.read_json(cached_dataset)
        else:
            user_dataset = UserDataset.objects.get(id=self.dataset_id)
            self.dataset = pd.read_csv(user_dataset.file.path)
            self.cache_manager.set_cached_response(cache_key, self.dataset.to_json())
```

STEP 4.2: Summary Statistics Tool (ai/analyticabd/tools/summary_statistics_tool.py)
```python
from .base_tool import BaseTool
import pandas as pd
import numpy as np

class SummaryStatisticsTool(BaseTool):
    def get_description(self):
        return "Generate comprehensive summary statistics including descriptive stats, data quality metrics, and distribution insights"
    
    def get_parameters_schema(self):
        return {
            "include_visualizations": {"type": "boolean", "default": True},
            "outlier_threshold": {"type": "float", "default": 1.5},
            "missing_threshold": {"type": "float", "default": 0.1}
        }
    
    def execute(self, parameters=None):
        cached_results = self.cache_manager.get_cached_tool_results(
            self.tool_name, self.dataset_id, parameters
        )
        if cached_results:
            return cached_results
        
        self.load_dataset()
        
        summary = {
            'dataset_overview': self._get_dataset_overview(),
            'variable_summary': self._get_variable_summary(),
            'data_quality': self._get_data_quality(parameters),
            'distribution_insights': self._get_distribution_insights(parameters),
            'correlation_matrix': self._get_correlation_matrix(),
            'outlier_analysis': self._get_outlier_analysis(parameters),
            'missing_data_analysis': self._get_missing_data_analysis(),
            'statistical_tests': self._get_statistical_tests()
        }
        
        self.cache_manager.cache_tool_results(
            self.tool_name, self.dataset_id, parameters, summary
        )
        
        return summary
    
    def _get_dataset_overview(self):
        return {
            'total_rows': len(self.dataset),
            'total_columns': len(self.dataset.columns),
            'memory_usage': self.dataset.memory_usage(deep=True).sum(),
            'data_types': self.dataset.dtypes.to_dict(),
            'sample_data': self.dataset.head(5).to_dict('records')
        }
    
    def _get_variable_summary(self):
        summary = {}
        for col in self.dataset.columns:
            col_data = self.dataset[col]
            if pd.api.types.is_numeric_dtype(col_data):
                summary[col] = {
                    'type': 'numeric',
                    'count': col_data.count(),
                    'mean': col_data.mean(),
                    'std': col_data.std(),
                    'min': col_data.min(),
                    'q25': col_data.quantile(0.25),
                    'median': col_data.median(),
                    'q75': col_data.quantile(0.75),
                    'max': col_data.max(),
                    'skewness': col_data.skew(),
                    'kurtosis': col_data.kurtosis()
                }
            else:
                summary[col] = {
                    'type': 'categorical',
                    'count': col_data.count(),
                    'unique_count': col_data.nunique(),
                    'most_common': col_data.mode().iloc[0] if not col_data.mode().empty else None,
                    'most_common_count': col_data.value_counts().iloc[0] if not col_data.empty else 0
                }
        return summary
    
    def _get_data_quality(self, parameters):
        missing_threshold = parameters.get('missing_threshold', 0.1) if parameters else 0.1
        
        quality_metrics = {}
        for col in self.dataset.columns:
            missing_pct = self.dataset[col].isnull().sum() / len(self.dataset)
            quality_metrics[col] = {
                'missing_percentage': missing_pct,
                'completeness': 1 - missing_pct,
                'quality_score': 1 - missing_pct if missing_pct < missing_threshold else 0.5,
                'issues': []
            }
            
            if missing_pct > missing_threshold:
                quality_metrics[col]['issues'].append('High missing data')
            
            if pd.api.types.is_numeric_dtype(self.dataset[col]):
                Q1 = self.dataset[col].quantile(0.25)
                Q3 = self.dataset[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((self.dataset[col] < (Q1 - 1.5 * IQR)) | 
                           (self.dataset[col] > (Q3 + 1.5 * IQR))).sum()
                quality_metrics[col]['outliers_count'] = outliers
                quality_metrics[col]['outliers_percentage'] = outliers / len(self.dataset)
        
        return quality_metrics
    
    def _get_distribution_insights(self, parameters):
        insights = {}
        for col in self.dataset.columns:
            if pd.api.types.is_numeric_dtype(self.dataset[col]):
                col_data = self.dataset[col].dropna()
                if len(col_data) > 0:
                    insights[col] = {
                        'distribution_type': 'normal' if abs(col_data.skew()) < 0.5 else 'skewed',
                        'skewness': col_data.skew(),
                        'kurtosis': col_data.kurtosis(),
                        'range': col_data.max() - col_data.min(),
                        'coefficient_of_variation': col_data.std() / col_data.mean() if col_data.mean() != 0 else 0
                    }
        return insights
    
    def _get_correlation_matrix(self):
        numeric_cols = self.dataset.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            correlation_matrix = self.dataset[numeric_cols].corr()
            return {
                'matrix': correlation_matrix.to_dict(),
                'strong_correlations': self._find_strong_correlations(correlation_matrix)
            }
        return {'matrix': {}, 'strong_correlations': []}
    
    def _find_strong_correlations(self, corr_matrix, threshold=0.7):
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) >= threshold:
                    strong_correlations.append({
                        'variable1': corr_matrix.columns[i],
                        'variable2': corr_matrix.columns[j],
                        'correlation': corr_value
                    })
        return strong_correlations
    
    def _get_outlier_analysis(self, parameters):
        outlier_threshold = parameters.get('outlier_threshold', 1.5) if parameters else 1.5
        outliers = {}
        
        for col in self.dataset.select_dtypes(include=[np.number]).columns:
            col_data = self.dataset[col].dropna()
            if len(col_data) > 0:
                Q1 = col_data.quantile(0.25)
                Q3 = col_data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - outlier_threshold * IQR
                upper_bound = Q3 + outlier_threshold * IQR
                
                outlier_indices = ((col_data < lower_bound) | (col_data > upper_bound))
                outliers[col] = {
                    'count': outlier_indices.sum(),
                    'percentage': outlier_indices.sum() / len(col_data),
                    'lower_bound': lower_bound,
                    'upper_bound': upper_bound,
                    'outlier_values': col_data[outlier_indices].tolist()
                }
        
        return outliers
    
    def _get_missing_data_analysis(self):
        missing_analysis = {}
        for col in self.dataset.columns:
            missing_count = self.dataset[col].isnull().sum()
            missing_analysis[col] = {
                'missing_count': missing_count,
                'missing_percentage': missing_count / len(self.dataset),
                'completeness': 1 - (missing_count / len(self.dataset))
            }
        return missing_analysis
    
    def _get_statistical_tests(self):
        tests = {}
        numeric_cols = self.dataset.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            col_data = self.dataset[col].dropna()
            if len(col_data) > 30:
                from scipy import stats
                shapiro_stat, shapiro_p = stats.shapiro(col_data)
                tests[col] = {
                    'normality_test': {
                        'test_name': 'Shapiro-Wilk',
                        'statistic': shapiro_stat,
                        'p_value': shapiro_p,
                        'is_normal': shapiro_p > 0.05
                    }
                }
        
        return tests
```

STEP 4.3: Tool Registry (ai/analyticabd/tools/tool_registry.py)
```python
class ToolRegistry:
    def __init__(self):
        self.tools = {}
        self._register_all_tools()
    
    def _register_all_tools(self):
        from .summary_statistics_tool import SummaryStatisticsTool
        from .correlation_tool import CorrelationTool
        from .regression_tool import RegressionTool
        from .clustering_tool import ClusteringTool
        from .time_series_tool import TimeSeriesTool
        from .outlier_detection_tool import OutlierDetectionTool
        from .hypothesis_testing_tool import HypothesisTestingTool
        from .data_quality_tool import DataQualityTool
        from .visualization_tool import VisualizationTool
        
        self.tools.update({
            'summary_statistics': SummaryStatisticsTool,
            'correlation': CorrelationTool,
            'regression': RegressionTool,
            'clustering': ClusteringTool,
            'time_series': TimeSeriesTool,
            'outlier_detection': OutlierDetectionTool,
            'hypothesis_testing': HypothesisTestingTool,
            'data_quality': DataQualityTool,
            'visualization': VisualizationTool
        })
    
    def get_tool(self, tool_name, dataset_id, user_id):
        if tool_name in self.tools:
            return self.tools[tool_name](dataset_id, user_id)
        return None
    
    def list_available_tools(self):
        return list(self.tools.keys())
    
    def get_tool_descriptions(self):
        descriptions = {}
        for tool_name, tool_class in self.tools.items():
            temp_tool = tool_class(0, 0)
            descriptions[tool_name] = temp_tool.get_description()
        return descriptions
```

================================================================================
PHASE 5: LLM CLIENT & CONVERSATION MANAGEMENT
================================================================================

STEP 5.1: LLM Client (ai/analyticabd/ai/llm_client.py)
```python
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, SystemMessage
from .cache_manager import CacheManager

class LLMClient:
    def __init__(self):
        self.api_key = os.getenv('GOOGLE_API_KEY')
        self.model = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            google_api_key=self.api_key,
            temperature=0.1,
            max_tokens=4000
        )
        self.cache_manager = CacheManager()
    
    def chat(self, message, context=None):
        messages = []
        system_prompt = self._build_system_prompt(context)
        messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=message))
        response = self.model.invoke(messages)
        return response.content
    
    def interpret_summary_statistics(self, dataset_id, summary_results):
        from ..models import UserDataset
        
        try:
            dataset = UserDataset.objects.get(id=dataset_id)
            context = {
                'dataset_name': dataset.name,
                'dataset_rows': dataset.rows,
                'dataset_columns': dataset.columns,
                'summary_results': summary_results
            }
            
            prompt = f"""
            Please interpret the following summary statistics for dataset '{dataset.name}' 
            with {dataset.rows} rows and {dataset.columns} columns:
            
            {summary_results}
            
            Provide insights about:
            1. Data quality and completeness
            2. Key patterns in the data
            3. Potential issues or anomalies
            4. Recommendations for further analysis
            """
            
            return self.chat(prompt, context)
        
        except UserDataset.DoesNotExist:
            return "Error: Dataset not found"
    
    def _build_system_prompt(self, context=None):
        base_prompt = """You are an AI analytics assistant for DataFlow Analytics. 
        You can help users analyze their datasets using various statistical tools.
        
        Available tools:
        - summary_statistics: Generate comprehensive summary statistics
        - correlation: Perform correlation analysis
        - regression: Perform regression analysis
        - clustering: Perform clustering analysis
        - time_series: Perform time series analysis
        - outlier_detection: Detect outliers in data
        - hypothesis_testing: Perform statistical hypothesis tests
        - data_quality: Assess data quality metrics
        - visualization: Create data visualizations
        
        When a user asks for analysis, you can:
        1. Suggest appropriate tools
        2. Execute tools and interpret results
        3. Provide insights and recommendations
        4. Guide users through the analysis process
        
        Always be helpful, clear, and provide actionable insights."""
        
        if context:
            base_prompt += f"\n\nContext: {context}"
        
        return base_prompt
```

================================================================================
PHASE 6: DATABASE MODELS FOR AI INTEGRATION
================================================================================

STEP 6.1: Chat Models (ai/analyticabd/models/chat_models.py)
```python
from django.db import models
from django.contrib.auth.models import User
import uuid

class ChatSession(models.Model):
    session_id = models.UUIDField(default=uuid.uuid4, unique=True)
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)
    
    class Meta:
        ordering = ['-created_at']

class ChatMessage(models.Model):
    ROLE_CHOICES = [
        ('user', 'User'),
        ('assistant', 'Assistant'),
        ('system', 'System'),
    ]
    
    session = models.ForeignKey(ChatSession, on_delete=models.CASCADE, related_name='messages')
    role = models.CharField(max_length=10, choices=ROLE_CHOICES)
    content = models.TextField()
    metadata = models.JSONField(default=dict, blank=True)
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        ordering = ['created_at']

class ToolExecution(models.Model):
    session = models.ForeignKey(ChatSession, on_delete=models.CASCADE, related_name='tool_executions')
    tool_name = models.CharField(max_length=100)
    dataset_id = models.IntegerField()
    parameters = models.JSONField(default=dict, blank=True)
    results = models.JSONField(default=dict, blank=True)
    created_at = models.DateTimeField(auto_now_add=True)
    execution_time = models.FloatField(null=True, blank=True)
    
    class Meta:
        ordering = ['-created_at']
```

================================================================================
NEXT STEPS - SEE PART 2
================================================================================

This completes Part 1 of the revised plan. Part 2 will include:
- Remaining tool implementations (Correlation, Regression, Clustering, etc.)
- Comprehensive API endpoints
- Frontend integration
- Testing and deployment strategies
- Future enhancements (n8n integration, messaging apps)
